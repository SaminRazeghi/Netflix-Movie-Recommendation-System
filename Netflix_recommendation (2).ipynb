{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import shutil\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "from surprise import Dataset, Reader, SVD, KNNBasic, NMF\n",
        "from surprise.model_selection import cross_validate\n",
        "from surprise.accuracy import rmse, mae\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.model_selection import train_test_split\n"
      ],
      "metadata": {
        "id": "iZND-dmfp0Y0"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "id": "AE-okL2_sKho",
        "outputId": "6ac3578c-2d82-4f3d-92ab-6b6e1326ded8"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-f3fcb5d9-ac1b-4073-88c3-4bd5c1da3d6a\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-f3fcb5d9-ac1b-4073-88c3-4bd5c1da3d6a\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle (6).json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle (6).json': b'{\"username\":\"saminrazeghi\",\"key\":\"d35cdf9ab3da4c583007f59bf86b7e49\"}'}"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "TjO5HuJusU1Y"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "netflix_dir = \"/content/netflix_data\"\n",
        "required_file = os.path.join(netflix_dir, \"combined_data_1.txt\")  # just check the first file\n",
        "\n",
        "# Check if data is already downloaded\n",
        "if not os.path.exists(required_file):\n",
        "    print(\"Netflix dataset not found. Downloading from Kaggle...\")\n",
        "    os.makedirs(netflix_dir, exist_ok=True)\n",
        "    !kaggle datasets download -d netflix-inc/netflix-prize-data -p /content/netflix_data --unzip\n",
        "    print(\"Download and extraction complete.\")\n",
        "else:\n",
        "    print(\"Netflix dataset already exists. Skipping download.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CmIe6s0YsPh1",
        "outputId": "b0837437-81f1-4bda-996a-e0a3f4765905"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Netflix dataset already exists. Skipping download.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Step 1: Loading the Netflix dataset...\")\n",
        "# Function to load ratings data into a DataFrame\n",
        "def load_ratings_to_dataframe(file_path, max_rows=100000):\n",
        "    \"\"\"Load Netflix ratings data into a pandas DataFrame\"\"\"\n",
        "    print(f\"Loading ratings from {file_path}...\")\n",
        "\n",
        "    # Parse ratings data\n",
        "    data = []\n",
        "    current_movie = None\n",
        "    count = 0\n",
        "\n",
        "    with open(file_path, 'r') as f:\n",
        "        for line in tqdm(f, desc=\"Loading ratings\"):\n",
        "            line = line.strip()\n",
        "\n",
        "            if line.endswith(':'):\n",
        "                current_movie = int(line[:-1])\n",
        "            else:\n",
        "                user_id, rating, date = line.split(',')\n",
        "                data.append([current_movie, int(user_id), int(rating), date])\n",
        "\n",
        "                count += 1\n",
        "                if count >= max_rows:\n",
        "                    break\n",
        "\n",
        "    # Create DataFrame\n",
        "    ratings_df = pd.DataFrame(data, columns=['movie_id', 'user_id', 'rating', 'date'])\n",
        "\n",
        "    print(f\"\\nLoaded {len(ratings_df)} ratings from {ratings_df['movie_id'].nunique()} movies\")\n",
        "    print(ratings_df.head())\n",
        "\n",
        "    return ratings_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D7wM8evJeJGL",
        "outputId": "f26fc3c3-70b2-41f9-839d-2041144d5714"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1: Loading the Netflix dataset...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to load movie titles into a DataFrame\n",
        "def load_movie_titles_to_dataframe(file_path):\n",
        "    \"\"\"Load Netflix movie titles into a pandas DataFrame\"\"\"\n",
        "    print(f\"Loading movie titles from {file_path}...\")\n",
        "\n",
        "    # Read file line by line\n",
        "    data = []\n",
        "    with open(file_path, 'r', encoding='latin1') as f:\n",
        "        for line in tqdm(f, desc=\"Loading titles\"):\n",
        "            parts = line.strip().split(',')\n",
        "\n",
        "            if len(parts) >= 1:\n",
        "                movie_id = int(parts[0])\n",
        "\n",
        "                # Extract year if available\n",
        "                year = None\n",
        "                if len(parts) >= 2 and parts[1].strip():\n",
        "                    try:\n",
        "                        year = int(parts[1])\n",
        "                    except:\n",
        "                        year = None\n",
        "\n",
        "                # Extract title if available\n",
        "                title = \"\"\n",
        "                if len(parts) >= 3:\n",
        "                    title = ','.join(parts[2:])\n",
        "\n",
        "                data.append([movie_id, year, title])\n",
        "\n",
        "    # Create DataFrame\n",
        "    titles_df = pd.DataFrame(data, columns=['movie_id', 'year', 'title'])\n",
        "\n",
        "    print(f\"\\nLoaded {len(titles_df)} movie titles\")\n",
        "    print(titles_df.head())\n",
        "\n",
        "    return titles_df"
      ],
      "metadata": {
        "id": "JtUxfiigeO3_"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data\n",
        "dataset_path = \"/content/netflix_data\"\n",
        "ratings_file = os.path.join(dataset_path, 'combined_data_1.txt')\n",
        "titles_file = os.path.join(dataset_path, 'movie_titles.csv')"
      ],
      "metadata": {
        "id": "Wsnk5MGpeTQF"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if files exist and load them\n",
        "if os.path.exists(ratings_file):\n",
        "    ratings_df = load_ratings_to_dataframe(ratings_file)\n",
        "else:\n",
        "    print(f\"Ratings file not found at {ratings_file}\")\n",
        "    ratings_df = None\n",
        "\n",
        "if os.path.exists(titles_file):\n",
        "    titles_df = load_movie_titles_to_dataframe(titles_file)\n",
        "else:\n",
        "    print(f\"Titles file not found at {titles_file}\")\n",
        "    titles_df = None"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xD3Wjq-Qecos",
        "outputId": "0363e4e3-18ce-426f-efcd-0eb0d5c282d1"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading ratings from /content/netflix_data/combined_data_1.txt...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading ratings: 100029it [00:00, 292623.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loaded 100000 ratings from 30 movies\n",
            "   movie_id  user_id  rating        date\n",
            "0         1  1488844       3  2005-09-06\n",
            "1         1   822109       5  2005-05-13\n",
            "2         1   885013       4  2005-10-19\n",
            "3         1    30878       4  2005-12-26\n",
            "4         1   823519       3  2004-05-03\n",
            "Loading movie titles from /content/netflix_data/movie_titles.csv...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading titles: 17770it [00:00, 643528.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loaded 17770 movie titles\n",
            "   movie_id    year                         title\n",
            "0         1  2003.0               Dinosaur Planet\n",
            "1         2  2004.0    Isle of Man TT 2004 Review\n",
            "2         3  1997.0                     Character\n",
            "3         4  1994.0  Paula Abdul's Get Up & Dance\n",
            "4         5  2004.0      The Rise and Fall of ECW\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lh /content/netflix_data\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ByONGhwCplWr",
        "outputId": "7c2b310e-200b-4f16-dc24-970f61505a1d"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 2.0G\n",
            "drwxr-xr-x 4 root root 4.0K Apr 22 18:51 app\n",
            "-rw-r--r-- 1 root root 473M Apr 22 18:26 combined_data_1.txt\n",
            "-rw-r--r-- 1 root root 530M Apr 22 18:26 combined_data_2.txt\n",
            "-rw-r--r-- 1 root root 444M Apr 22 18:26 combined_data_3.txt\n",
            "-rw-r--r-- 1 root root 527M Apr 22 18:27 combined_data_4.txt\n",
            "drwxr-xr-x 2 root root 4.0K Apr 22 18:14 data\n",
            "drwxr-xr-x 2 root root 4.0K Apr 22 18:17 figures\n",
            "drwxr-xr-x 2 root root 4.0K Apr 22 18:27 models\n",
            "-rw-r--r-- 1 root root 565K Apr 22 18:27 movie_titles.csv\n",
            "-rw-r--r-- 1 root root 2.3M Apr 22 18:14 netflix_sample.csv\n",
            "-rw-r--r-- 1 root root  11M Apr 22 18:27 probe.txt\n",
            "-rw-r--r-- 1 root root  51M Apr 22 18:27 qualifying.txt\n",
            "-rw-r--r-- 1 root root 5.8K Apr 22 18:26 README\n",
            "-rw-r--r-- 1 root root  900 Apr 22 19:14 sample_recommendations.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nStep 2: Creating a sample dataset for analysis...\")\n",
        "# Prepare a sample dataset for the recommendation system\n",
        "sample_path = os.path.join(dataset_path, 'netflix_sample.csv')\n",
        "\n",
        "# Create sample ratings file if it doesn't exist\n",
        "if not os.path.exists(sample_path) and ratings_df is not None:\n",
        "    print(f\"Creating sample ratings file at {sample_path}\")\n",
        "\n",
        "    # Convert date to datetime format\n",
        "    ratings_df['date'] = pd.to_datetime(ratings_df['date'])\n",
        "\n",
        "    # Save to CSV\n",
        "    ratings_df.to_csv(sample_path, index=False)\n",
        "    print(f\"Saved sample ratings to {sample_path}\")\n",
        "else:\n",
        "    print(f\"Sample ratings file already exists at {sample_path}\")\n",
        "    # Load the sample if it exists\n",
        "    ratings_df = pd.read_csv(sample_path)\n",
        "    ratings_df['date'] = pd.to_datetime(ratings_df['date'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXwNk4__ejd7",
        "outputId": "aee8c169-c6af-4d54-984a-027d13faf6ae"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 2: Creating a sample dataset for analysis...\n",
            "Sample ratings file already exists at /content/netflix_data/netflix_sample.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create necessary directories\n",
        "models_dir = os.path.join(dataset_path, 'models')\n",
        "data_dir = os.path.join(dataset_path, 'data')\n",
        "app_dir = os.path.join(dataset_path, 'app')\n",
        "app_data_dir = os.path.join(app_dir, 'data')\n",
        "app_models_dir = os.path.join(app_dir, 'models')\n",
        "fig_dir = os.path.join(dataset_path, 'figures')\n",
        "\n",
        "for directory in [models_dir, data_dir, app_dir, app_data_dir, app_models_dir, fig_dir]:\n",
        "    os.makedirs(directory, exist_ok=True)\n",
        "\n",
        "# Copy sample to app directory\n",
        "if not os.path.exists(os.path.join(app_data_dir, 'netflix_sample.csv')):\n",
        "    shutil.copy(sample_path, os.path.join(app_data_dir, 'netflix_sample.csv'))\n",
        "    print(f\"Copied sample data to {os.path.join(app_data_dir, 'netflix_sample.csv')}\")\n",
        "\n",
        "# Copy titles to app directory\n",
        "if titles_df is not None and not os.path.exists(os.path.join(app_data_dir, 'movie_titles.csv')):\n",
        "    titles_df.to_csv(os.path.join(app_data_dir, 'movie_titles.csv'), index=False)\n",
        "    print(f\"Saved movie titles to {os.path.join(app_data_dir, 'movie_titles.csv')}\")\n",
        "\n",
        "print(\"Sample dataset preparation complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCp2kjyQepTT",
        "outputId": "25cb1d19-1d08-405e-c81b-d3a4000a5537"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample dataset preparation complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nStep 3: Preprocessing the data...\")\n",
        "# Preprocess the Netflix dataset\n",
        "def preprocess_netflix_data(ratings_df, titles_df):\n",
        "    \"\"\"\n",
        "    Preprocess the Netflix dataset for modeling.\n",
        "\n",
        "    Args:\n",
        "        ratings_df: DataFrame with ratings\n",
        "        titles_df: DataFrame with movie titles\n",
        "\n",
        "    Returns:\n",
        "        Processed DataFrame ready for modeling\n",
        "    \"\"\"\n",
        "    print(\"Preprocessing data...\")\n",
        "\n",
        "    # Make sure date is datetime\n",
        "    if not pd.api.types.is_datetime64_dtype(ratings_df['date']):\n",
        "        ratings_df['date'] = pd.to_datetime(ratings_df['date'])\n",
        "\n",
        "    # Add temporal features\n",
        "    ratings_df['year'] = ratings_df['date'].dt.year\n",
        "    ratings_df['month'] = ratings_df['date'].dt.month\n",
        "    ratings_df['day'] = ratings_df['date'].dt.day\n",
        "    ratings_df['day_of_week'] = ratings_df['date'].dt.dayofweek\n",
        "\n",
        "    # Merge with movie metadata\n",
        "    processed_df = ratings_df.merge(titles_df, on='movie_id', how='left', suffixes=('', '_movie'))\n",
        "\n",
        "    # Rename year column from movie titles to avoid conflict\n",
        "    if 'year_movie' in processed_df.columns:\n",
        "        processed_df = processed_df.rename(columns={'year_movie': 'release_year'})\n",
        "\n",
        "    # Create user features\n",
        "    user_features = ratings_df.groupby('user_id').agg({\n",
        "        'rating': ['mean', 'std', 'count'],\n",
        "        'movie_id': 'nunique'\n",
        "    })\n",
        "\n",
        "    user_features.columns = ['user_avg_rating', 'user_rating_std', 'user_rating_count', 'user_movie_count']\n",
        "    user_features = user_features.reset_index()\n",
        "\n",
        "    # Create movie features\n",
        "    movie_features = ratings_df.groupby('movie_id').agg({\n",
        "        'rating': ['mean', 'std', 'count'],\n",
        "        'user_id': 'nunique'\n",
        "    })\n",
        "\n",
        "    movie_features.columns = ['movie_avg_rating', 'movie_rating_std', 'movie_rating_count', 'movie_user_count']\n",
        "    movie_features = movie_features.reset_index()\n",
        "\n",
        "    # Merge features back to the main dataframe\n",
        "    processed_df = processed_df.merge(user_features, on='user_id', how='left')\n",
        "    processed_df = processed_df.merge(movie_features, on='movie_id', how='left')\n",
        "\n",
        "    print(\"Preprocessing complete.\")\n",
        "    print(f\"Processed dataframe shape: {processed_df.shape}\")\n",
        "    print(processed_df.head())\n",
        "\n",
        "    return processed_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-sYr0NzetQI",
        "outputId": "bb41198e-bec0-4bca-e5c8-4854be30935f"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 3: Preprocessing the data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Process data\n",
        "if ratings_df is not None and titles_df is not None:\n",
        "    processed_df = preprocess_netflix_data(ratings_df, titles_df)\n",
        "else:\n",
        "    print(\"Cannot preprocess data - missing ratings or titles data\")\n",
        "    processed_df = None\n",
        "\n",
        "print(\"\\nStep 4: Performing exploratory data analysis...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZM2YmSeewPj",
        "outputId": "680f14de-a2aa-4261-98bc-bc0716adb859"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing data...\n",
            "Preprocessing complete.\n",
            "Processed dataframe shape: (100000, 18)\n",
            "   movie_id  user_id  rating       date  year  month  day  day_of_week  \\\n",
            "0         1  1488844       3 2005-09-06  2005      9    6            1   \n",
            "1         1   822109       5 2005-05-13  2005      5   13            4   \n",
            "2         1   885013       4 2005-10-19  2005     10   19            2   \n",
            "3         1    30878       4 2005-12-26  2005     12   26            0   \n",
            "4         1   823519       3 2004-05-03  2004      5    3            0   \n",
            "\n",
            "   release_year            title  user_avg_rating  user_rating_std  \\\n",
            "0        2003.0  Dinosaur Planet              3.0         0.816497   \n",
            "1        2003.0  Dinosaur Planet              5.0              NaN   \n",
            "2        2003.0  Dinosaur Planet              4.5         0.707107   \n",
            "3        2003.0  Dinosaur Planet              3.0         1.224745   \n",
            "4        2003.0  Dinosaur Planet              3.4         1.516575   \n",
            "\n",
            "   user_rating_count  user_movie_count  movie_avg_rating  movie_rating_std  \\\n",
            "0                  4                 4          3.749543           1.06792   \n",
            "1                  1                 1          3.749543           1.06792   \n",
            "2                  2                 2          3.749543           1.06792   \n",
            "3                  5                 5          3.749543           1.06792   \n",
            "4                  5                 5          3.749543           1.06792   \n",
            "\n",
            "   movie_rating_count  movie_user_count  \n",
            "0                 547               547  \n",
            "1                 547               547  \n",
            "2                 547               547  \n",
            "3                 547               547  \n",
            "4                 547               547  \n",
            "\n",
            "Step 4: Performing exploratory data analysis...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Explore and visualize the Netflix dataset\n",
        "def explore_netflix_data(df):\n",
        "    \"\"\"\n",
        "    Perform exploratory data analysis on the Netflix dataset.\n",
        "\n",
        "    Args:\n",
        "        df: Preprocessed DataFrame\n",
        "    \"\"\"\n",
        "    print(\"Performing exploratory data analysis...\")\n",
        "\n",
        "    # Set plot style\n",
        "    plt.style.use('default')  # Using default style to avoid version issues\n",
        "\n",
        "    # Basic dataset information\n",
        "    print(\"\\nDataset Overview:\")\n",
        "    print(f\"Total ratings: {len(df)}\")\n",
        "    print(f\"Unique users: {df['user_id'].nunique()}\")\n",
        "    print(f\"Unique movies: {df['movie_id'].nunique()}\")\n",
        "    print(f\"Rating range: {df['rating'].min()} to {df['rating'].max()}\")\n",
        "    if 'date' in df.columns:\n",
        "        print(f\"Date range: {df['date'].min()} to {df['date'].max()}\")\n",
        "\n",
        "    # Rating distribution\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.countplot(x='rating', data=df)\n",
        "    plt.title('Distribution of Ratings')\n",
        "    plt.xlabel('Rating')\n",
        "    plt.ylabel('Count')\n",
        "    plt.savefig(os.path.join(fig_dir, 'rating_distribution.png'))\n",
        "    plt.close()\n",
        "\n",
        "    # User activity distribution\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    user_activity = df.groupby('user_id')['rating'].count()\n",
        "    sns.histplot(user_activity, bins=50, kde=True)\n",
        "    plt.title('Distribution of Ratings per User')\n",
        "    plt.xlabel('Number of Ratings')\n",
        "    plt.ylabel('Number of Users')\n",
        "    plt.xscale('log')\n",
        "    plt.savefig(os.path.join(fig_dir, 'user_activity.png'))\n",
        "    plt.close()\n",
        "\n",
        "    # Movie popularity distribution\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    movie_popularity = df.groupby('movie_id')['rating'].count()\n",
        "    sns.histplot(movie_popularity, bins=50, kde=True)\n",
        "    plt.title('Distribution of Ratings per Movie')\n",
        "    plt.xlabel('Number of Ratings')\n",
        "    plt.ylabel('Number of Movies')\n",
        "    plt.xscale('log')\n",
        "    plt.savefig(os.path.join(fig_dir, 'movie_popularity.png'))\n",
        "    plt.close()\n",
        "\n",
        "    # Average rating by movie popularity\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    movie_stats = df.groupby('movie_id').agg({'rating': ['mean', 'count']})\n",
        "    movie_stats.columns = ['avg_rating', 'count']\n",
        "    movie_stats = movie_stats.reset_index()\n",
        "\n",
        "    plt.scatter(movie_stats['count'], movie_stats['avg_rating'], alpha=0.5)\n",
        "    plt.title('Average Rating vs. Movie Popularity')\n",
        "    plt.xlabel('Number of Ratings')\n",
        "    plt.ylabel('Average Rating')\n",
        "    plt.xscale('log')\n",
        "    plt.savefig(os.path.join(fig_dir, 'rating_vs_popularity.png'))\n",
        "    plt.close()\n",
        "\n",
        "    print(\"EDA complete. Figures saved to:\", fig_dir)"
      ],
      "metadata": {
        "id": "q8PI5mW1e19e"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Explore the data\n",
        "if processed_df is not None:\n",
        "    explore_netflix_data(processed_df)\n",
        "else:\n",
        "    print(\"Cannot perform EDA - processed data is not available\")\n",
        "\n",
        "print(\"\\nStep 5: Creating train/test split...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sovg-Rkke5DW",
        "outputId": "85a3e1a3-258b-4991-d324-8171206960c2"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing exploratory data analysis...\n",
            "\n",
            "Dataset Overview:\n",
            "Total ratings: 100000\n",
            "Unique users: 81490\n",
            "Unique movies: 30\n",
            "Rating range: 1 to 5\n",
            "Date range: 2000-01-06 00:00:00 to 2005-12-31 00:00:00\n",
            "EDA complete. Figures saved to: /content/netflix_data/figures\n",
            "\n",
            "Step 5: Creating train/test split...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create train/test split\n",
        "def create_train_test_split(df, method='time', test_size=0.2):\n",
        "    \"\"\"\n",
        "    Split the dataset into training and testing sets.\n",
        "\n",
        "    Args:\n",
        "        df: Preprocessed DataFrame\n",
        "        method: 'random' or 'time' based splitting\n",
        "        test_size: Proportion of data for testing\n",
        "\n",
        "    Returns:\n",
        "        train_df, test_df: Training and testing DataFrames\n",
        "    \"\"\"\n",
        "    print(f\"Creating {method}-based train/test split with test_size={test_size}...\")\n",
        "\n",
        "    if method == 'time' and 'date' in df.columns:\n",
        "        # Sort by timestamp\n",
        "        df = df.sort_values('date')\n",
        "\n",
        "        # Use oldest data for training, newest for testing\n",
        "        train_size = int((1 - test_size) * len(df))\n",
        "        train_df = df.iloc[:train_size]\n",
        "        test_df = df.iloc[train_size:]\n",
        "    else:\n",
        "        # Random split\n",
        "        train_df, test_df = train_test_split(df, test_size=test_size, random_state=42)\n",
        "\n",
        "    print(f\"Train set: {len(train_df)} samples\")\n",
        "    print(f\"Test set: {len(test_df)} samples\")\n",
        "\n",
        "    return train_df, test_df"
      ],
      "metadata": {
        "id": "37J6nQike7UY"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data\n",
        "if processed_df is not None:\n",
        "    train_df, test_df = create_train_test_split(processed_df, method='time')\n",
        "else:\n",
        "    print(\"Cannot create train/test split - processed data is not available\")\n",
        "    train_df, test_df = None, None"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "csVnGLWwe9Qd",
        "outputId": "27fc60ed-30a2-4a6d-cb52-f1dbd2e772f6"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating time-based train/test split with test_size=0.2...\n",
            "Train set: 80000 samples\n",
            "Test set: 20000 samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def save_and_copy_model(model_obj, name, models_dir, app_models_dir):\n",
        "    # Save to models_dir\n",
        "    model_path = os.path.join(models_dir, f\"{name}_model.pkl\")\n",
        "    with open(model_path, 'wb') as f:\n",
        "        pickle.dump(model_obj, f)\n",
        "    print(f\"{name} model saved to {model_path}\")\n",
        "\n",
        "    # Copy to app_models_dir\n",
        "    os.makedirs(app_models_dir, exist_ok=True)\n",
        "    app_model_path = os.path.join(app_models_dir, f\"{name}_model.pkl\")\n",
        "    shutil.copy(model_path, app_model_path)\n",
        "    print(f\"Copied to app directory at {app_model_path}\")"
      ],
      "metadata": {
        "id": "bs6e-PHnxUYr"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nStep 6: Building recommendation models...\")\n",
        "# Build recommendation models\n",
        "def build_recommendation_models(train_df, test_df):\n",
        "    \"\"\"\n",
        "    Build and evaluate multiple recommendation models.\n",
        "\n",
        "    Args:\n",
        "        train_df: Training DataFrame\n",
        "        test_df: Testing DataFrame\n",
        "\n",
        "    Returns:\n",
        "        Dictionary of trained models and their performance metrics\n",
        "    \"\"\"\n",
        "    print(\"Building recommendation models...\")\n",
        "\n",
        "    # Convert dataframes to Surprise format\n",
        "    reader = Reader(rating_scale=(1, 5))\n",
        "\n",
        "    train_data = Dataset.load_from_df(train_df[['user_id', 'movie_id', 'rating']], reader)\n",
        "    train_set = train_data.build_full_trainset()\n",
        "\n",
        "    # Prepare test data\n",
        "    test_set = [(row.user_id, row.movie_id, row.rating)\n",
        "                for _, row in test_df[['user_id', 'movie_id', 'rating']].iterrows()]\n",
        "\n",
        "    # Initialize models\n",
        "    models = {\n",
        "        'SVD': SVD(n_factors=50, n_epochs=10, lr_all=0.005, reg_all=0.02),\n",
        "        'KNN': KNNBasic(k=40, sim_options={'name': 'pearson_baseline', 'user_based': False}),\n",
        "        'NMF': NMF(n_factors=50, n_epochs=20)\n",
        "    }\n",
        "\n",
        "    # Train and evaluate each model\n",
        "    results = {}\n",
        "\n",
        "    for name, model in models.items():\n",
        "        print(f\"\\nTraining {name} model...\")\n",
        "        model.fit(train_set)\n",
        "\n",
        "        # Make predictions on test set\n",
        "        print(f\"Evaluating {name} model...\")\n",
        "        predictions = model.test(test_set)\n",
        "\n",
        "        # Calculate RMSE\n",
        "        rmse_score = rmse(predictions)\n",
        "        mae_score = mae(predictions)\n",
        "\n",
        "        results[name] = {\n",
        "            'model': model,\n",
        "            'rmse': rmse_score,\n",
        "            'mae': mae_score,\n",
        "            'predictions': predictions\n",
        "        }\n",
        "\n",
        "        print(f\"{name} Results: RMSE = {rmse_score:.4f}, MAE = {mae_score:.4f}\")\n",
        "\n",
        "    # Train a simple baseline model (global mean)\n",
        "    global_mean = train_df['rating'].mean()\n",
        "    baseline_rmse = np.sqrt(np.mean((test_df['rating'] - global_mean) ** 2))\n",
        "    baseline_mae = np.mean(np.abs(test_df['rating'] - global_mean))\n",
        "\n",
        "    results['Baseline'] = {\n",
        "        'model': None,\n",
        "        'rmse': baseline_rmse,\n",
        "        'mae': baseline_mae,\n",
        "        'predictions': None\n",
        "    }\n",
        "\n",
        "    print(f\"\\nBaseline Results: RMSE = {baseline_rmse:.4f}, MAE = {baseline_mae:.4f}\")\n",
        "\n",
        "    for name, model in models.items():\n",
        "        print(f\"\\nTraining {name} model...\")\n",
        "        model.fit(train_set)\n",
        "\n",
        "        print(f\"Evaluating {name} model...\")\n",
        "        predictions = model.test(test_set)\n",
        "\n",
        "        rmse_score = rmse(predictions)\n",
        "        mae_score = mae(predictions)\n",
        "\n",
        "        results[name] = {\n",
        "            'model': model,\n",
        "            'rmse': rmse_score,\n",
        "            'mae': mae_score,\n",
        "            'predictions': predictions\n",
        "        }\n",
        "\n",
        "        print(f\"{name} Results: RMSE = {rmse_score:.4f}, MAE = {mae_score:.4f}\")\n",
        "\n",
        "        # Save and copy model to both locations\n",
        "        save_and_copy_model(model, name.lower(), models_dir, app_models_dir)\n",
        "\n",
        "\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Afv5uT8fCfT",
        "outputId": "5ea5b156-30d1-4087-c40f-a48a2773435b"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 6: Building recommendation models...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build recommendation models\n",
        "if train_df is not None and test_df is not None:\n",
        "    model_results = build_recommendation_models(train_df, test_df)\n",
        "else:\n",
        "    print(\"Cannot build models - train/test data is not available\")\n",
        "    model_results = {}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-dZMMyTfEX3",
        "outputId": "be607e76-3ad2-40b4-9e69-c89a8d398e0a"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building recommendation models...\n",
            "\n",
            "Training SVD model...\n",
            "Evaluating SVD model...\n",
            "RMSE: 1.0665\n",
            "MAE:  0.8359\n",
            "SVD Results: RMSE = 1.0665, MAE = 0.8359\n",
            "\n",
            "Training KNN model...\n",
            "Estimating biases using als...\n",
            "Computing the pearson_baseline similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Evaluating KNN model...\n",
            "RMSE: 1.1896\n",
            "MAE:  0.9735\n",
            "KNN Results: RMSE = 1.1896, MAE = 0.9735\n",
            "\n",
            "Training NMF model...\n",
            "Evaluating NMF model...\n",
            "RMSE: 1.2651\n",
            "MAE:  1.0339\n",
            "NMF Results: RMSE = 1.2651, MAE = 1.0339\n",
            "\n",
            "Baseline Results: RMSE = 1.1451, MAE = 0.9578\n",
            "\n",
            "Training SVD model...\n",
            "Evaluating SVD model...\n",
            "RMSE: 1.0673\n",
            "MAE:  0.8364\n",
            "SVD Results: RMSE = 1.0673, MAE = 0.8364\n",
            "svd model saved to /content/netflix_data/models/svd_model.pkl\n",
            "Copied to app directory at /content/netflix_data/app/models/svd_model.pkl\n",
            "\n",
            "Training KNN model...\n",
            "Estimating biases using als...\n",
            "Computing the pearson_baseline similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Evaluating KNN model...\n",
            "RMSE: 1.1896\n",
            "MAE:  0.9735\n",
            "KNN Results: RMSE = 1.1896, MAE = 0.9735\n",
            "knn model saved to /content/netflix_data/models/knn_model.pkl\n",
            "Copied to app directory at /content/netflix_data/app/models/knn_model.pkl\n",
            "\n",
            "Training NMF model...\n",
            "Evaluating NMF model...\n",
            "RMSE: 1.2604\n",
            "MAE:  1.0298\n",
            "NMF Results: RMSE = 1.2604, MAE = 1.0298\n",
            "nmf model saved to /content/netflix_data/models/nmf_model.pkl\n",
            "Copied to app directory at /content/netflix_data/app/models/nmf_model.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# First, define all classes at the module level\n",
        "class ContentBasedRecommender:\n",
        "    def __init__(self, similarity_matrix, indices, movie_data):\n",
        "        self.similarity_matrix = similarity_matrix\n",
        "        self.indices = indices\n",
        "        self.movie_data = movie_data\n",
        "\n",
        "    def get_recommendations(self, movie_id, top_n=10):\n",
        "        \"\"\"Get content-based recommendations for a movie\"\"\"\n",
        "        # Get the index of the movie\n",
        "        if movie_id not in self.indices:\n",
        "            return []\n",
        "\n",
        "        idx = self.indices[movie_id]\n",
        "\n",
        "        # Get similarity scores\n",
        "        sim_scores = list(enumerate(self.similarity_matrix[idx]))\n",
        "\n",
        "        # Sort movies based on similarity scores\n",
        "        sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        # Get top N most similar movies\n",
        "        sim_scores = sim_scores[1:top_n+1]\n",
        "\n",
        "        # Get movie indices\n",
        "        movie_indices = [i[0] for i in sim_scores]\n",
        "\n",
        "        # Return movie IDs and similarity scores\n",
        "        return [(self.movie_data.iloc[i]['movie_id'], sim_scores[j][1])\n",
        "                for j, i in enumerate(movie_indices)]\n",
        "\n",
        "class HybridRecommender:\n",
        "    def __init__(self, cf_model, content_recommender, cf_weight=0.7):\n",
        "        self.cf_model = cf_model\n",
        "        self.content_recommender = content_recommender\n",
        "        self.cf_weight = cf_weight\n",
        "\n",
        "    def predict(self, user_id, movie_id):\n",
        "        \"\"\"Predict rating for a user-movie pair using hybrid approach\"\"\"\n",
        "        try:\n",
        "            # Get collaborative filtering prediction\n",
        "            cf_pred = self.cf_model.predict(user_id, movie_id).est\n",
        "            # Normalize to 0-1 scale\n",
        "            cf_pred_norm = (cf_pred - 1) / 4\n",
        "            # Default content score\n",
        "            content_score = 0.5\n",
        "            # Return weighted prediction\n",
        "            return self.cf_weight * cf_pred_norm + (1 - self.cf_weight) * content_score\n",
        "        except:\n",
        "            # Return default score if prediction fails\n",
        "            return 0.5\n",
        "\n",
        "# Now use the globally defined class in your building function\n",
        "def build_content_based_model(titles_df):\n",
        "    \"\"\"\n",
        "    Build a content-based recommendation model using movie metadata.\n",
        "\n",
        "    Args:\n",
        "        titles_df: Movie titles DataFrame\n",
        "\n",
        "    Returns:\n",
        "        Content-based model dictionary\n",
        "    \"\"\"\n",
        "    print(\"Building content-based filtering model...\")\n",
        "\n",
        "    # Create a text representation of movie features\n",
        "    titles_df['text_features'] = titles_df['title'].fillna('')\n",
        "\n",
        "    if 'year' in titles_df.columns:\n",
        "        # Convert year to string and combine with title\n",
        "        titles_df['text_features'] = titles_df['text_features'] + ' ' + titles_df['year'].fillna('').astype(str)\n",
        "\n",
        "    # Create TF-IDF vectors\n",
        "    tfidf = TfidfVectorizer(stop_words='english')\n",
        "    tfidf_matrix = tfidf.fit_transform(titles_df['text_features'])\n",
        "\n",
        "    # Compute similarity matrix\n",
        "    cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
        "\n",
        "    # Create a mapping from movie IDs to indices\n",
        "    indices = pd.Series(titles_df.index, index=titles_df['movie_id']).drop_duplicates()\n",
        "\n",
        "    # Create the recommender object\n",
        "    recommender = ContentBasedRecommender(cosine_sim, indices, titles_df)\n",
        "\n",
        "    print(\"Content-based model built successfully.\")\n",
        "\n",
        "    return {\n",
        "        'model': recommender,\n",
        "        'similarity_matrix': cosine_sim,\n",
        "        'indices': indices,\n",
        "        'tfidf': tfidf\n",
        "    }\n",
        "\n",
        "# Build content-based model\n",
        "if titles_df is not None:\n",
        "    content_model = build_content_based_model(titles_df)\n",
        "else:\n",
        "    print(\"Cannot build content-based model - movie titles data is not available\")\n",
        "    content_model = None\n",
        "\n",
        "print(\"\\nStep 8: Building hybrid recommendation system...\")\n",
        "\n",
        "# Build hybrid model\n",
        "if 'SVD' in model_results and content_model is not None:\n",
        "    try:\n",
        "        print(\"Building hybrid recommendation model...\")\n",
        "        # Extract the SVD model\n",
        "        svd_model = model_results['SVD']['model']\n",
        "\n",
        "        # Extract the content-based model\n",
        "        content_recommender = content_model['model']\n",
        "\n",
        "        # Create the hybrid recommender instance\n",
        "        hybrid_recommender = HybridRecommender(svd_model, content_recommender)\n",
        "\n",
        "        # Create model for pickling\n",
        "        pickle_model = {\n",
        "            'svd_model': svd_model,\n",
        "            'content_model': content_recommender,\n",
        "            'cf_weight': 0.7\n",
        "        }\n",
        "\n",
        "        # Save the model\n",
        "        os.makedirs(models_dir, exist_ok=True)\n",
        "        model_path = os.path.join(models_dir, 'hybrid_model.pkl')\n",
        "        with open(model_path, 'wb') as f:\n",
        "            pickle.dump(pickle_model, f)\n",
        "\n",
        "        # Copy to app directory\n",
        "        os.makedirs(app_models_dir, exist_ok=True)\n",
        "        app_model_path = os.path.join(app_models_dir, 'hybrid_model.pkl')\n",
        "        shutil.copy(model_path, app_model_path)\n",
        "\n",
        "        print(f\"Hybrid model saved to {model_path}\")\n",
        "        print(f\"Copied to app directory at {app_model_path}\")\n",
        "\n",
        "        # Store full hybrid model for use in the current session\n",
        "        hybrid_model = {\n",
        "            'svd_model': svd_model,\n",
        "            'content_model': content_recommender,\n",
        "            'hybrid_recommender': hybrid_recommender,\n",
        "            'cf_weight': 0.7\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error building hybrid model: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()  # Print the full stack trace\n",
        "        hybrid_model = None\n",
        "else:\n",
        "    print(\"Cannot build hybrid model - missing SVD model or content model\")\n",
        "    hybrid_model = None"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aOMj1pikfLrF",
        "outputId": "ea00b41e-1f8b-45fb-ad27-f6d8eab0157d"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building content-based filtering model...\n",
            "Content-based model built successfully.\n",
            "\n",
            "Step 8: Building hybrid recommendation system...\n",
            "Building hybrid recommendation model...\n",
            "Hybrid model saved to /content/netflix_data/models/hybrid_model.pkl\n",
            "Copied to app directory at /content/netflix_data/app/models/hybrid_model.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nStep 9: Creating evaluation metrics visualization...\")\n",
        "# Visualize model results\n",
        "def visualize_model_results(model_results):\n",
        "    \"\"\"\n",
        "    Visualize the performance of different recommendation models.\n",
        "    \"\"\"\n",
        "    print(\"Visualizing model results...\")\n",
        "\n",
        "    # Create figures directory if it doesn't exist\n",
        "    fig_dir = os.path.join(dataset_path, 'figures')\n",
        "    os.makedirs(fig_dir, exist_ok=True)\n",
        "\n",
        "    # RMSE comparison\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    rmse_values = {name: result['rmse'] for name, result in model_results.items() if 'rmse' in result}\n",
        "    if rmse_values:\n",
        "        plt.bar(rmse_values.keys(), rmse_values.values())\n",
        "        plt.title('RMSE Comparison of Different Models')\n",
        "        plt.ylabel('RMSE (lower is better)')\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(fig_dir, 'rmse_comparison.png'))\n",
        "        plt.close()\n",
        "        print(f\"RMSE comparison chart saved to {os.path.join(fig_dir, 'rmse_comparison.png')}\")\n",
        "\n",
        "    # MAE comparison\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    mae_values = {name: result['mae'] for name, result in model_results.items() if 'mae' in result}\n",
        "    if mae_values:\n",
        "        plt.bar(mae_values.keys(), mae_values.values())\n",
        "        plt.title('MAE Comparison of Different Models')\n",
        "        plt.ylabel('MAE (lower is better)')\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(fig_dir, 'mae_comparison.png'))\n",
        "        plt.close()\n",
        "        print(f\"MAE comparison chart saved to {os.path.join(fig_dir, 'mae_comparison.png')}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-DswHScahTmz",
        "outputId": "acd2840e-aea1-45e7-f1bd-1102c6b69cef"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 9: Creating evaluation metrics visualization...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize model results\n",
        "if model_results:\n",
        "    visualize_model_results(model_results)\n",
        "else:\n",
        "    print(\"Cannot visualize results - no model results available\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-olfW_O8hVMr",
        "outputId": "b5509398-b40e-494d-d40f-5b257dfe4898"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visualizing model results...\n",
            "RMSE comparison chart saved to /content/netflix_data/figures/rmse_comparison.png\n",
            "MAE comparison chart saved to /content/netflix_data/figures/mae_comparison.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create sample recommendations\n",
        "def create_sample_recommendations(model_results, processed_df, titles_df):\n",
        "    \"\"\"\n",
        "    Create sample recommendations for demo purposes.\n",
        "    \"\"\"\n",
        "    if not model_results or processed_df is None or titles_df is None:\n",
        "        print(\"Cannot create sample recommendations - missing required data\")\n",
        "        return\n",
        "\n",
        "    print(\"Creating sample recommendations...\")\n",
        "\n",
        "    # Choose a random user\n",
        "    sample_user_id = processed_df['user_id'].sample(1).iloc[0]\n",
        "\n",
        "    # Get movies rated by this user\n",
        "    user_movies = processed_df[processed_df['user_id'] == sample_user_id]\n",
        "    print(f\"Sample recommendations for user {sample_user_id}\")\n",
        "    print(f\"User has rated {len(user_movies)} movies\")\n",
        "\n",
        "    # Show user's top-rated movies\n",
        "    top_rated = user_movies.sort_values('rating', ascending=False).head(5)\n",
        "    print(\"\\nUser's top-rated movies:\")\n",
        "    for _, row in top_rated.iterrows():\n",
        "        movie_info = titles_df[titles_df['movie_id'] == row['movie_id']]\n",
        "        if not movie_info.empty:\n",
        "            movie = movie_info.iloc[0]\n",
        "            print(f\"  - {movie['title']} ({movie.get('year', 'N/A')}): {row['rating']}/5\")\n",
        "\n",
        "    # Get recommendations from each model\n",
        "    print(\"\\nRecommendations from different models:\")\n",
        "\n",
        "    for model_name, result in model_results.items():\n",
        "        if model_name == 'Baseline' or 'model' not in result or result['model'] is None:\n",
        "            continue\n",
        "\n",
        "        print(f\"\\n{model_name} Recommendations:\")\n",
        "\n",
        "        # Get all movies not rated by this user\n",
        "        rated_movies = set(user_movies['movie_id'])\n",
        "        all_movies = set(processed_df['movie_id'].unique())\n",
        "        unrated_movies = list(all_movies - rated_movies)\n",
        "\n",
        "        # Limit to 100 random unrated movies for efficiency\n",
        "        if len(unrated_movies) > 100:\n",
        "            unrated_movies = np.random.choice(unrated_movies, 100, replace=False)\n",
        "\n",
        "        # Make predictions for unrated movies\n",
        "        predictions = []\n",
        "        model = result['model']\n",
        "\n",
        "        for movie_id in unrated_movies:\n",
        "            try:\n",
        "                pred = model.predict(sample_user_id, movie_id)\n",
        "                predictions.append((movie_id, pred.est))\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "        # Sort by predicted rating and get top 5\n",
        "        predictions.sort(key=lambda x: x[1], reverse=True)\n",
        "        top_recs = predictions[:5]\n",
        "\n",
        "        # Display recommendations\n",
        "        for movie_id, score in top_recs:\n",
        "            movie_info = titles_df[titles_df['movie_id'] == movie_id]\n",
        "            if not movie_info.empty:\n",
        "                movie = movie_info.iloc[0]\n",
        "                print(f\"  - {movie['title']} ({movie.get('year', 'N/A')}): {score:.2f}/5\")\n",
        "\n",
        "    # Save sample recommendations to file\n",
        "    recommendations_file = os.path.join(dataset_path, 'sample_recommendations.txt')\n",
        "    with open(recommendations_file, 'w') as f:\n",
        "        f.write(f\"Sample recommendations for user {sample_user_id}\\n\")\n",
        "        f.write(f\"User has rated {len(user_movies)} movies\\n\\n\")\n",
        "\n",
        "        f.write(\"User's top-rated movies:\\n\")\n",
        "        for _, row in top_rated.iterrows():\n",
        "            movie_info = titles_df[titles_df['movie_id'] == row['movie_id']]\n",
        "            if not movie_info.empty:\n",
        "                movie = movie_info.iloc[0]\n",
        "                f.write(f\"  - {movie['title']} ({movie.get('year', 'N/A')}): {row['rating']}/5\\n\")\n",
        "\n",
        "        for model_name, result in model_results.items():\n",
        "            if model_name == 'Baseline' or 'model' not in result or result['model'] is None:\n",
        "                continue\n",
        "\n",
        "            f.write(f\"\\n{model_name} Recommendations:\\n\")\n",
        "\n",
        "            for movie_id, score in top_recs:\n",
        "                movie_info = titles_df[titles_df['movie_id'] == movie_id]\n",
        "                if not movie_info.empty:\n",
        "                    movie = movie_info.iloc[0]\n",
        "                    f.write(f\"  - {movie['title']} ({movie.get('year', 'N/A')}): {score:.2f}/5\\n\")\n",
        "\n",
        "    print(f\"\\nSample recommendations saved to {recommendations_file}\")"
      ],
      "metadata": {
        "id": "pzBSX2Dqhbji"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create sample recommendations\n",
        "if model_results and processed_df is not None and titles_df is not None:\n",
        "    create_sample_recommendations(model_results, processed_df, titles_df)\n",
        "else:\n",
        "    print(\"Cannot create sample recommendations - missing required data\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0CO7Zup2hdCu",
        "outputId": "7fb4facd-16c4-47a3-ab9a-f9fc1abe4b27"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating sample recommendations...\n",
            "Sample recommendations for user 449866\n",
            "User has rated 1 movies\n",
            "\n",
            "User's top-rated movies:\n",
            "  - Immortal Beloved (1994.0): 3/5\n",
            "\n",
            "Recommendations from different models:\n",
            "\n",
            "SVD Recommendations:\n",
            "  - Lord of the Rings: The Return of the King: Extended Edition: Bonus Material (2003.0): 4.39/5\n",
            "  - Inspector Morse 31: Death Is Now My Neighbour (1997.0): 4.03/5\n",
            "  - The Rise and Fall of ECW (2004.0): 4.00/5\n",
            "  - Lilo and Stitch (2002.0): 3.84/5\n",
            "  - Isle of Man TT 2004 Review (2004.0): 3.82/5\n",
            "\n",
            "KNN Recommendations:\n",
            "  - Paula Abdul's Get Up & Dance (1994.0): 3.55/5\n",
            "  - Class of Nuke 'Em High 2 (1991.0): 3.55/5\n",
            "  - Chump Change (2000.0): 3.55/5\n",
            "  - Dinosaur Planet (2003.0): 3.00/5\n",
            "  - Isle of Man TT 2004 Review (2004.0): 3.00/5\n",
            "\n",
            "NMF Recommendations:\n",
            "  - Lord of the Rings: The Return of the King: Extended Edition: Bonus Material (2003.0): 4.73/5\n",
            "  - Inspector Morse 31: Death Is Now My Neighbour (1997.0): 4.71/5\n",
            "  - The Rise and Fall of ECW (2004.0): 4.39/5\n",
            "  - Something's Gotta Give (2003.0): 4.37/5\n",
            "  - Character (1997.0): 4.33/5\n",
            "\n",
            "Sample recommendations saved to /content/netflix_data/sample_recommendations.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nStep 11: Creating Streamlit demo application...\")\n",
        "# Create Streamlit demo application\n",
        "# Update the create_streamlit_demo function to work with the modified models\n",
        "def create_streamlit_demo():\n",
        "    \"\"\"\n",
        "    Create a Streamlit demo application script.\n",
        "    \"\"\"\n",
        "    demo_path = os.path.join(dataset_path, 'app', 'app.py')\n",
        "    os.makedirs(os.path.dirname(demo_path), exist_ok=True)\n",
        "\n",
        "    with open(demo_path, 'w') as f:\n",
        "        f.write('''\n",
        "\n",
        "# First, define all classes at the module level\n",
        "class ContentBasedRecommender:\n",
        "    def __init__(self, similarity_matrix, indices, movie_data):\n",
        "        self.similarity_matrix = similarity_matrix\n",
        "        self.indices = indices\n",
        "        self.movie_data = movie_data\n",
        "\n",
        "    def get_recommendations(self, movie_id, top_n=10):\n",
        "        \"\"\"Get content-based recommendations for a movie\"\"\"\n",
        "        # Get the index of the movie\n",
        "        if movie_id not in self.indices:\n",
        "            return []\n",
        "\n",
        "        idx = self.indices[movie_id]\n",
        "\n",
        "        # Get similarity scores\n",
        "        sim_scores = list(enumerate(self.similarity_matrix[idx]))\n",
        "\n",
        "        # Sort movies based on similarity scores\n",
        "        sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        # Get top N most similar movies\n",
        "        sim_scores = sim_scores[1:top_n+1]\n",
        "\n",
        "        # Get movie indices\n",
        "        movie_indices = [i[0] for i in sim_scores]\n",
        "\n",
        "        # Return movie IDs and similarity scores\n",
        "        return [(self.movie_data.iloc[i]['movie_id'], sim_scores[j][1])\n",
        "                for j, i in enumerate(movie_indices)]\n",
        "\n",
        "class HybridRecommender:\n",
        "    def __init__(self, cf_model, content_recommender, cf_weight=0.7):\n",
        "        self.cf_model = cf_model\n",
        "        self.content_recommender = content_recommender\n",
        "        self.cf_weight = cf_weight\n",
        "\n",
        "    def predict(self, user_id, movie_id):\n",
        "        \"\"\"Predict rating for a user-movie pair using hybrid approach\"\"\"\n",
        "        try:\n",
        "            # Get collaborative filtering prediction\n",
        "            cf_pred = self.cf_model.predict(user_id, movie_id).est\n",
        "            # Normalize to 0-1 scale\n",
        "            cf_pred_norm = (cf_pred - 1) / 4\n",
        "            # Default content score\n",
        "            content_score = 0.5\n",
        "            # Return weighted prediction\n",
        "            return self.cf_weight * cf_pred_norm + (1 - self.cf_weight) * content_score\n",
        "        except:\n",
        "            # Return default score if prediction fails\n",
        "            return 0.5\n",
        "\n",
        "\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import dill as pickle\n",
        "import os\n",
        "\n",
        "\n",
        "\n",
        "# Set page title and configuration\n",
        "st.set_page_config(\n",
        "    page_title=\"Netflix Recommendation System\",\n",
        "    page_icon=\"\",\n",
        "    layout=\"wide\"\n",
        ")\n",
        "\n",
        "# Title and introduction\n",
        "st.title(\" Netflix Recommendation System\")\n",
        "st.markdown(\"\"\"\n",
        "This application demonstrates a hybrid recommendation system built using the Netflix Prize dataset.\n",
        "You can select a user and see personalized movie recommendations based on different algorithms.\n",
        "\"\"\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def load_model_and_data():\n",
        "    # Load model from relative path\n",
        "    model_path = './app/models/hybrid_model.pkl'\n",
        "\n",
        "    if not os.path.exists(model_path):\n",
        "        st.error(\"Model file not found. Please ensure the model has been trained and saved correctly.\")\n",
        "        return None, None, None\n",
        "\n",
        "    # Load model\n",
        "    with open(model_path, 'rb') as f:\n",
        "        st.write(\" Loading model...\")\n",
        "        model_data = pickle.load(f)\n",
        "\n",
        "    # Load sample ratings data\n",
        "    ratings_df = pd.read_csv('./data/netflix_sample.csv')\n",
        "\n",
        "    # Load movie titles\n",
        "    titles_df = pd.read_csv('./data/movie_titles.csv',\n",
        "                           header=None,\n",
        "                           names=['movie_id', 'year', 'title'],\n",
        "                           encoding='latin1')\n",
        "\n",
        "    return model_data, ratings_df, titles_df\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Load data\n",
        "model_data, ratings_df, titles_df = load_model_and_data()\n",
        "\n",
        "# Check if data loaded successfully\n",
        "if model_data is None or ratings_df is None or titles_df is None:\n",
        "    st.warning(\"Please upload the required data files or check file paths.\")\n",
        "    st.stop()\n",
        "\n",
        "# Create sidebar\n",
        "st.sidebar.header(\"Settings\")\n",
        "\n",
        "# User selection\n",
        "sample_users = ratings_df['user_id'].value_counts().head(100).index.tolist()\n",
        "selected_user = st.sidebar.selectbox(\"Select a user ID\", sample_users)\n",
        "\n",
        "# Algorithm selection\n",
        "algorithm = st.sidebar.radio(\n",
        "    \"Select recommendation algorithm\",\n",
        "    [\"Collaborative Filtering (SVD)\", \"Content-Based\", \"Hybrid\"]\n",
        ")\n",
        "\n",
        "# Number of recommendations\n",
        "num_recs = st.sidebar.slider(\"Number of recommendations\", 5, 20, 10)\n",
        "\n",
        "# Function to get SVD recommendations\n",
        "def get_svd_recommendations(user_id, n=10):\n",
        "    # Get the SVD model\n",
        "    svd_model = model_data['svd_model']\n",
        "\n",
        "    # Get movies already rated by this user\n",
        "    rated_movies = set(ratings_df[ratings_df['user_id'] == user_id]['movie_id'])\n",
        "\n",
        "    # Get all movies\n",
        "    all_movies = set(ratings_df['movie_id'].unique())\n",
        "\n",
        "    # Movies to predict\n",
        "    movies_to_predict = list(all_movies - rated_movies)\n",
        "\n",
        "    # Predict ratings\n",
        "    predictions = []\n",
        "    for movie_id in movies_to_predict:\n",
        "        try:\n",
        "            pred = svd_model.predict(user_id, movie_id)\n",
        "            predictions.append((movie_id, pred.est))\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    # Sort by predicted rating and get top n\n",
        "    predictions.sort(key=lambda x: x[1], reverse=True)\n",
        "    return predictions[:n]\n",
        "\n",
        "# Function to get content-based recommendations\n",
        "def get_content_recommendations(user_id, n=10):\n",
        "    # Get the content model\n",
        "    content_recommender = model_data['content_model']['model']\n",
        "\n",
        "    # Get user's top-rated movies\n",
        "    user_ratings = ratings_df[ratings_df['user_id'] == user_id].sort_values('rating', ascending=False)\n",
        "\n",
        "    # Get already rated movies\n",
        "    rated_movies = set(user_ratings['movie_id'])\n",
        "\n",
        "    # Get recommendations based on top movies\n",
        "    all_recs = []\n",
        "    for _, row in user_ratings.head(3).iterrows():\n",
        "        movie_id = row['movie_id']\n",
        "        recs = content_recommender.get_recommendations(movie_id)\n",
        "        all_recs.extend(recs)\n",
        "\n",
        "    # Remove already rated movies and sort by similarity\n",
        "    filtered_recs = [(movie_id, score) for movie_id, score in all_recs if movie_id not in rated_movies]\n",
        "\n",
        "    # Remove duplicates and sort\n",
        "    unique_recs = {}\n",
        "    for movie_id, score in filtered_recs:\n",
        "        if movie_id not in unique_recs:\n",
        "            unique_recs[movie_id] = score\n",
        "        else:\n",
        "            unique_recs[movie_id] = max(unique_recs[movie_id], score)\n",
        "\n",
        "    sorted_recs = sorted(unique_recs.items(), key=lambda x: x[1], reverse=True)\n",
        "    return sorted_recs[:n]\n",
        "\n",
        "# Function to get hybrid recommendations\n",
        "def get_hybrid_recommendations(user_id, n=10):\n",
        "    # Get collaborative filtering recommendations\n",
        "    cf_recs = get_svd_recommendations(user_id, n=n*2)\n",
        "\n",
        "    # Get content-based recommendations\n",
        "    content_recs = get_content_recommendations(user_id, n=n*2)\n",
        "\n",
        "    # Get the hybrid recommender if available\n",
        "    if 'hybrid_recommender' in model_data:\n",
        "        hybrid_recommender = model_data['hybrid_recommender']\n",
        "        cf_weight = model_data.get('cf_weight', 0.7)\n",
        "    else:\n",
        "        # Create a simple hybrid approach if no saved hybrid recommender\n",
        "        cf_weight = 0.7\n",
        "\n",
        "    # Combine recommendations\n",
        "    cf_dict = dict(cf_recs)\n",
        "    content_dict = dict(content_recs)\n",
        "\n",
        "    # Normalize CF scores (they're on a 1-5 scale)\n",
        "    cf_dict = {k: (v-1)/4 for k, v in cf_dict.items()}\n",
        "\n",
        "    # Get all movie IDs\n",
        "    all_movies = set(list(cf_dict.keys()) + list(content_dict.keys()))\n",
        "\n",
        "    # Compute hybrid scores\n",
        "    hybrid_scores = {}\n",
        "    for movie_id in all_movies:\n",
        "        cf_score = cf_dict.get(movie_id, 0.5)  # Default to middle score\n",
        "        content_score = content_dict.get(movie_id, 0.5)  # Default to middle score\n",
        "\n",
        "        # Weighted average\n",
        "        hybrid_score = cf_weight * cf_score + (1 - cf_weight) * content_score\n",
        "        hybrid_scores[movie_id] = hybrid_score\n",
        "\n",
        "    # Sort by hybrid score\n",
        "    sorted_hybrid = sorted(hybrid_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "    return sorted_hybrid[:n]\n",
        "\n",
        "# Function to display recommendations\n",
        "def display_recommendations(recommendations):\n",
        "    # Create columns for movie display\n",
        "    cols = st.columns(5)\n",
        "\n",
        "    for i, (movie_id, score) in enumerate(recommendations):\n",
        "        col_idx = i % 5\n",
        "\n",
        "        # Get movie info\n",
        "        movie_info = titles_df[titles_df['movie_id'] == movie_id]\n",
        "\n",
        "        if len(movie_info) > 0:\n",
        "            movie = movie_info.iloc[0]\n",
        "            title = movie['title']\n",
        "            year = movie.get('year', 'N/A')\n",
        "\n",
        "            # Display in appropriate column\n",
        "            with cols[col_idx]:\n",
        "                st.markdown(f\"**{title}** ({year})\")\n",
        "                st.progress(min(score, 1.0))  # Normalize score for progress bar\n",
        "\n",
        "                st.caption(f\"Score: {score:.2f}\")\n",
        "\n",
        "\n",
        "# Main section\n",
        "if st.sidebar.button(\"Generate Recommendations\"):\n",
        "    # Display user information\n",
        "    user_ratings = ratings_df[ratings_df['user_id'] == selected_user]\n",
        "\n",
        "    st.subheader(f\"User {selected_user} Profile\")\n",
        "\n",
        "    col1, col2 = st.columns(2)\n",
        "\n",
        "    with col1:\n",
        "        st.metric(\"Total Movies Rated\", len(user_ratings))\n",
        "        st.metric(\"Average Rating\", f\"{user_ratings['rating'].mean():.2f}/5\")\n",
        "\n",
        "    with col2:\n",
        "        # Rating distribution\n",
        "        rating_counts = user_ratings['rating'].value_counts().sort_index()\n",
        "        st.bar_chart(rating_counts)\n",
        "\n",
        "    # Show user's top rated movies\n",
        "    st.subheader(\"User's Top-Rated Movies\")\n",
        "    top_movies = user_ratings.sort_values('rating', ascending=False).head(5)\n",
        "\n",
        "    for _, row in top_movies.iterrows():\n",
        "        movie_info = titles_df[titles_df['movie_id'] == row['movie_id']]\n",
        "        if len(movie_info) > 0:\n",
        "            movie = movie_info.iloc[0]\n",
        "            st.markdown(f\"**{movie['title']}** ({movie.get('year', 'N/A')}) - Rating: {row['rating']}/5\")\n",
        "\n",
        "    # Generate recommendations based on selected algorithm\n",
        "    st.header(f\"Recommendations using {algorithm}\")\n",
        "\n",
        "    if algorithm == \"Collaborative Filtering (SVD)\":\n",
        "        recommendations = get_svd_recommendations(selected_user, n=num_recs)\n",
        "    elif algorithm == \"Content-Based\":\n",
        "        recommendations = get_content_recommendations(selected_user, n=num_recs)\n",
        "    else:  # Hybrid\n",
        "        recommendations = get_hybrid_recommendations(selected_user, n=num_recs)\n",
        "\n",
        "    display_recommendations(recommendations)\n",
        "\n",
        "    # Show explanation\n",
        "    st.subheader(\"How it works\")\n",
        "\n",
        "    if algorithm == \"Collaborative Filtering (SVD)\":\n",
        "        st.markdown(\"\"\"\n",
        "        **Collaborative Filtering** works by finding patterns in how users rate movies.\n",
        "        It identifies users with similar tastes to provide recommendations.\n",
        "\n",
        "        The SVD (Singular Value Decomposition) algorithm creates latent factors that represent both users and movies\n",
        "        in a shared mathematical space, allowing the system to predict how a user would rate a movie they haven't seen yet.\n",
        "        \"\"\")\n",
        "    elif algorithm == \"Content-Based\":\n",
        "        st.markdown(\"\"\"\n",
        "        **Content-Based Filtering** recommends movies similar to ones the user has highly rated in the past.\n",
        "\n",
        "        This approach analyzes movie attributes (like titles, genres, etc.) to find similar movies,\n",
        "        without relying on ratings from other users.\n",
        "        \"\"\")\n",
        "    else:  # Hybrid\n",
        "        st.markdown(\"\"\"\n",
        "        **Hybrid Recommendation** combines both collaborative filtering and content-based approaches.\n",
        "\n",
        "        This method leverages both the patterns in user ratings (collaborative filtering) and the\n",
        "        similarities between movies (content-based) to provide more robust recommendations.\n",
        "        \"\"\")\n",
        "\n",
        "else:\n",
        "    # Show instructions when first loading\n",
        "    st.info(\" Select a user and click 'Generate Recommendations' to get personalized movie recommendations.\")\n",
        "\n",
        "    # Show dataset information\n",
        "    st.subheader(\"About the Dataset\")\n",
        "    st.markdown(\"\"\"\n",
        "    The **Netflix Prize Dataset** contains over 100 million ratings from Netflix customers.\n",
        "    This demo uses a sample of that dataset to demonstrate different recommendation algorithms.\n",
        "\n",
        "    The full dataset includes:\n",
        "    - Ratings from approximately 480,000 users\n",
        "    - Ratings for around 17,700 movies\n",
        "    - Ratings on a scale from 1 to 5 stars\n",
        "    - Dates of when the ratings were made\n",
        "    \"\"\")\n",
        "\n",
        "# Footer\n",
        "st.sidebar.markdown(\"---\")\n",
        "st.sidebar.markdown(\"**Netflix Recommendation System Demo**\")\n",
        "st.sidebar.markdown(\"Created as a portfolio project\")\n",
        "''')\n",
        "\n",
        "\n",
        "    print(f\"Streamlit demo application created at {demo_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dzncs8pGeX1h",
        "outputId": "10ecb5a5-9a45-46b9-af4f-5ac762e631ae"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 11: Creating Streamlit demo application...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "create_streamlit_demo()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1u27ucFHn42O",
        "outputId": "794c8ab6-7529-4b00-cd09-b34520869969"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Streamlit demo application created at /content/netflix_data/app/app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l /content/drive/MyDrive/Netflix/netflix_dataset/app\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2DaBPeRon-Uw",
        "outputId": "77f65809-e840-4b59-ae0b-f6bf0a245ebe"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 16\n",
            "-rw-r--r-- 1 root root 9970 Apr 22 17:50 app.py\n",
            "drwxr-xr-x 2 root root 4096 Apr 22 17:31 models\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "ngrok.set_auth_token(\"***\")"
      ],
      "metadata": {
        "id": "wj1889ojnPa7"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import threading\n",
        "import time\n",
        "\n",
        "# Kill any existing tunnels\n",
        "ngrok.kill()\n",
        "\n",
        "# Path to your Streamlit app\n",
        "app_path = \"/content/drive/MyDrive/Netflix/netflix_dataset/app/app.py\"\n",
        "\n",
        "# Function to run Streamlit\n",
        "def run_streamlit():\n",
        "    os.system(f\"cd '{os.path.dirname(app_path)}' && streamlit run app.py\")\n",
        "\n",
        "# Start Streamlit in background thread\n",
        "thread = threading.Thread(target=run_streamlit)\n",
        "thread.start()\n",
        "\n",
        "# Wait for Streamlit to start\n",
        "time.sleep(5)  # Give Streamlit time to launch\n",
        "\n",
        "# Open ngrok tunnel to port 8501\n",
        "public_url = ngrok.connect(addr=\"8501\")\n",
        "print(f\" Streamlit app is live at: {public_url}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eptOkBMRoWi2",
        "outputId": "74e67289-69c0-48ce-9494-ea0a766070e6"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Streamlit app is live at: NgrokTunnel: \"https://ef04-34-106-9-163.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cIYbyEuB6cE5"
      },
      "execution_count": 67,
      "outputs": []
    }
  ]
}